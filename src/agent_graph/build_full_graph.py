from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START
from fireworks.client import ChatCompletion
from langchain_core.messages import AIMessage
from agent_graph.tool_chinook_sqlagent import query_chinook_sqldb
from agent_graph.tool_travel_sqlagent import query_travel_sqldb
from agent_graph.tool_lookup_policy_rag import lookup_swiss_airline_policy
from agent_graph.tool_tavily_search import search_tool
from agent_graph.tool_stories_rag import lookup_stories
from agent_graph.load_tools_config import LoadToolsConfig
from agent_graph.agent_backend import State, BasicToolNode, route_tools
from pydantic import create_model
import inspect
import json

TOOLS_CFG = LoadToolsConfig()

def jsonschema(f):
    """
    Generate an OpenAI-compatible JSON schema for a given function.

    This helper converts a Python function with type hints into a JSON Schema
    that describes its input parameters and can be used as a `function` definition
    for LLMs that support tool calling (e.g., Fireworks, OpenAI).

    Args:
        f (function): A Python function with annotated parameters.

    Returns:
        dict: A dictionary with the following keys:
            - name (str): The name of the function.
            - description (str): The function's docstring.
            - parameters (dict): The JSON schema of its arguments.
    """
    kw = {n: (o.annotation, ... if o.default == inspect.Parameter.empty else o.default)
          for n, o in inspect.signature(f).parameters.items()}
    s = create_model(f'Input for `{f.__name__}`', **kw).model_json_schema()  # Use model_json_schema for Pydantic v2+
    return dict(name=f.__name__, description=f.__doc__, parameters=s)

def build_graph():
    """
    Builds an agent decision-making graph by combining an LLM with various tools
    and defining the flow of interactions between them.

    This function sets up a state graph where a primary language model (LLM) interacts
    with several predefined tools (e.g., databases, search functions, policy lookup, etc.).
    The agent can invoke tools based on conditions and use their outputs to inform
    further decisions. The flow involves conditional tool invocation, returning back
    to the chatbot after tool execution to guide the next step.

    Steps:
    1. Initializes the primary language model (LLM) with tool-binding functionality.
    2. Defines nodes in the graph where each node represents a specific action:
       - Chatbot node: Executes the LLM with the given state and messages.
       - Tools node: Runs the tool invocations based on the last message in the input state.
    3. Implements conditional routing between the chatbot and tools:
       - If a tool is required, it routes to the tools node.
       - Otherwise, the flow ends.
    4. Establishes connections between the chatbot and tools nodes to form the agent loop.
    5. Uses a memory-saving mechanism to track and save checkpoints in the graph.

    Returns:
        graph (StateGraph): The compiled state graph that represents the decision-making process
        of the agent, integrating the chatbot, tools, and conditional routing.

    Components:
        - `primary_llm`: The primary language model responsible for generating responses.
        - `tools`: A list of tools including SQL queries, search functionalities, policy lookups, etc.
        - `tool_node`: A node responsible for handling tool execution based on the chatbot's request.
        - `chatbot`: A function that takes the state as input and returns a message generated by the LLM.
        - `route_tools`: A conditional function to determine whether the chatbot should call a tool.
        - `graph`: The complete graph with nodes and conditional edges.
    """
    graph_builder = StateGraph(State)

    # Load tools with their proper configs
    lookup_swiss_airline_policy_json = jsonschema(lookup_swiss_airline_policy.func)
    lookup_stories_json = jsonschema(lookup_stories.func)
    query_travel_sqldb_json = jsonschema(query_travel_sqldb.func)
    query_chinook_sqldb_json = jsonschema(query_chinook_sqldb.func)
    search_tool_json = jsonschema(search_tool.func)
    tools = [lookup_swiss_airline_policy_json, lookup_stories_json, query_travel_sqldb_json, query_chinook_sqldb_json, search_tool_json]
    
    def chatbot(state):
        # Convert state["messages"] to OpenAI-style messages
        messages = []
        for msg in state["messages"]:
            if isinstance(msg, dict):
                # Already in OpenAI format
                messages.append(msg)
            else:
                # Convert from LangChain message to dict
                if hasattr(msg, 'model_dump'):
                    msg_dict = msg.model_dump()
                else:
                    msg_dict = msg.__dict__
                
                # Filter to only include required fields
                if msg_dict.get('type') == 'ai' or msg_dict.get('role') == 'assistant':
                    # Handle assistant messages with tool calls
                    converted_msg = {
                        'role': 'assistant',
                        'content': msg_dict.get('content', '')
                    }
                    
                    # Handle tool calls if present
                    if 'tool_calls' in msg_dict and msg_dict['tool_calls']:
                        converted_tool_calls = []
                        for tool_call in msg_dict['tool_calls']:
                            if isinstance(tool_call, dict):
                                # Convert from LangChain format to Fireworks format
                                if 'name' in tool_call and 'args' in tool_call:
                                    converted_tool_calls.append({
                                        'function': {
                                            'name': tool_call['name'],
                                            'arguments': json.dumps(tool_call['args'])
                                        },
                                        'id': tool_call.get('id', '')
                                    })
                                elif 'function' in tool_call:
                                    # Already in correct format
                                    converted_tool_calls.append(tool_call)
                        
                        if converted_tool_calls:
                            converted_msg['tool_calls'] = converted_tool_calls
                    
                    messages.append(converted_msg)
                    
                elif msg_dict.get('type') == 'human' or msg_dict.get('role') == 'user':
                    messages.append({
                        'role': 'user',
                        'content': msg_dict.get('content', '')
                    })
                elif msg_dict.get('type') == 'tool' or msg_dict.get('role') == 'tool':
                    messages.append({
                        'role': 'tool',
                        'content': msg_dict.get('content', ''),
                        'tool_call_id': msg_dict.get('tool_call_id', '')
                    })
        
        # Tell the LLM which tools it can call
        response = ChatCompletion.create(
            model=TOOLS_CFG.primary_agent_llm,
            messages=messages,
            functions=tools,
            function_call="auto",
            temperature=TOOLS_CFG.primary_agent_llm_temperature
        )
        
        # Convert the response to LangChain format
        response_msg = response.choices[0].message
        
        # Convert to LangChain AIMessage
        
        # Handle tool calls if present
        if hasattr(response_msg, 'tool_calls') and response_msg.tool_calls:
            tool_calls = []
            for tool_call in response_msg.tool_calls:
                              
                # Convert Fireworks tool call to LangChain format
                if hasattr(tool_call, 'function'):
                    converted_tool_call = {
                        'name': tool_call.function.name,
                        'args': json.loads(tool_call.function.arguments),
                        'id': tool_call.id,
                        'type': 'tool_call'
                    }
                else:
                    # Already in dict format
                    converted_tool_call = tool_call
                
                tool_calls.append(converted_tool_call)
            
            ai_message = AIMessage(
                content=response_msg.content,
                tool_calls=tool_calls
            )
        else:
            ai_message = AIMessage(content=response_msg.content)
        
        return {"messages": [ai_message]}
    
    graph_builder.add_node("chatbot", chatbot)


    tool_node = BasicToolNode(
        tools=[
            search_tool,
            lookup_swiss_airline_policy,
            lookup_stories,
            query_travel_sqldb,
            query_chinook_sqldb,
        ])
    graph_builder.add_node("tools", tool_node)
    # The `tools_condition` function returns "tools" if the chatbot asks to use a tool, and "__end__" if
    # it is fine directly responding. This conditional routing defines the main agent loop.
    graph_builder.add_conditional_edges(
        "chatbot",
        route_tools,
        # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node
        # It defaults to the identity function, but if you
        # want to use a node named something else apart from "tools",
        # You can update the value of the dictionary to something else
        # e.g., "tools": "my_tools"
        {"tools": "tools", "__end__": "__end__"},
    )

    # Any time a tool is called, we return to the chatbot to decide the next step
    graph_builder.add_edge("tools", "chatbot")
    graph_builder.add_edge(START, "chatbot")
    memory = MemorySaver()
    graph = graph_builder.compile(checkpointer=memory)
    return graph
